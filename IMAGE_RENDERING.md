# Microscopy Image rendering

__Author's (Ilan's) note__:  This is my understanding of the problem and our solutions.  If I am mistaken, please feel free to make a pull request.  I had literally no background in imaging or rendering before taking this on, but continue to be exhiliarted as problems and solutions present themselves.

## Images and Computers

On some level, images can be thought of simply as `height x width x channels` matrices containing numbers as entries.  Computers have three little lights per pixel on the monitor, each of which can take on 256 different values (0-255).  For this and definitely other reasons, most images have three (or four) `channels`, RGB (or RGBA where A controls the alpha blending), each with 256 possible values.  However, this is somewhat arbitrary - our microscopy data, for example, can have many channels, each corresponding to a different antibody staining that is imaged, and each taking on a far greater range than the standard 256 options.  Beyond microscopy, many normal DSLR cameras can take photographs that [go beyond the 256 options as well](https://www.dpbestflow.org/camera/sensor#depth).  This data (in a standard 256 bit depth format) is then often compressed (in a [lossless, like .png files via DEFLATE](https://en.wikipedia.org/wiki/Portable_Network_Graphics#Compression), or in a [lossy, like .jpeg files with the Discrete Cosine Transform](https://en.wikipedia.org/wiki/JPEG#JPEG_codec_example), manner) and ready to be transported over the internet or opened on a computer (in both cases, the data must be first decompressed before it can be viewed).

We have thus established that standard image files are 8-bit (256 value-options) `height x width x 3` or `height x width x 4` matrices, often compressed.  Our data, as noted, is different, often being 16-bit or 32-bit and taking on more than the standard 3 or 4 channels (or possibly less).

## Viewing High Resolution Images

A challenge of working in a browser is that you don't have access to data unless it is fed over the internet to you.  Thus if you have a very large image in `height` and `width`, storing the entire image in memory is tough.  For this reason, people have developed various image __tiling__/__pyramimd__ schemes. __Tiling__ is the manner in which the underlying image is broken up into smaller images and the __pyramid__ represents the image at increasingly downsampled resolutions.  This allows you to efficiently view the image in different locations at different zoom levels, given the current viewport in your browser, the most famous example of this being [OpenSeaDragon](https://openseadragon.github.io/).

![alt text](deepzoom_example.jpg)

 For example, if you are very zoomed in on an image, say the top-left corner of `0:512` in both height and width on the original image, you only need that portion of the image served to your browser for you to view it.  You don't need the rest of the image at this moment and should you pan to it, you can always fetch it from the server which contains the output of the __tiling__/__pyramid__ generation.

## Our Data: Problems and solutions

We thus have two main problems: our data is often in a non-standard format, and it is very large.  Therefore, we need efficient methods of rendering and serving.

#### 1. Rendering >256 bits of data per channel, with more than 3 channels

To tackle this we look to [WebGL](https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API), a Javascript API for using OpenGL in the browser, as the core technology and [DeckGL](https://deck.gl/#/) as the high-level API for our application to communicate with WebGL.  WebGL is very flexible and will allow us to pass down data (i.e a flattened image matrix) with 16 and 32 bit precision to __shaders__, which can do graphics rendering and basic arithmetic using the GPU very efficiently.    This extended range of 16 or 32 bits can thus be limited by the user to a subset of it and mapped down to a "0 to 1" range that WebGL can render. Additionally, we can pass down a flag to tell WebGL which color each channel should be mapped to.

#### 2. Serving Data Efficiently

Most high resolution image viewing systems rely on `jpeg` files, which can obtain 10:1 compression ratios with imperceptible loss of quality, to serve the image tiles.  The `jpeg` files are very small and therefore there is minimal bottleneck with network requests.  However, this file type is [not an option](https://caniuse.com/#feat=jpegxr) (realistically) for more than 8 bits of data per channel in current browsers (they do not ship with the decompression algorithm for more than 8 bit data), nor is it clear that we wish to lose any precision in what is essentially a scientific measurement.  The other popular image file type, `png`, achieves considerably worse compression (around 2:1) but is lossless.

Our approach to solving this is twofold.  

First, we store the image in a compressed, "raw" format that can be decoded in the browser.  For this, we currently use [zarr](https://zarr.readthedocs.io/en/stable/) with [zarr.js](https://github.com/gzuidhof/zarr.js).  __Zarr__ essentially stores matrices of data using the same algorithm as png (`DEFLATE`) but very flexibly, in that the data can be of any size, in any shape.  For our purposes, it is best to store the data as flattened image tiles, which can then be read in the client by __Zarr.js__.  Taking a step back to see why this is necessary, let us note that __WebGL__ uses flat arrays and offset parameters to know how to scan the data so it can be passed down to the shaders - therefore pre-flattening the data before it reaches the client will result in best performance.  Thus, we flatten each image array of the pyramid in `tileSize x tileSize` passes over the data using [numpy](https://numpy.org/) and then store this in a __Zarr__ file folder for efficient use.  For example, if you have a `1028x1028x3` image with `tileSize` equal to 128, the data would be converted into a `10 x 10 x 128 x 128 x 3` numpy array (i.e the top `128x128` entries of the original image becomes a block, then the next `128x128` entries and so forth), and then flattened in [row-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order) to be served to the client.

Second, we make use of [HTTP2](https://en.wikipedia.org/wiki/HTTP/2) to speed up the requests.  Even though the requests are not particularly large (normally less than 250kb per tile with a `512x512` tile size), there are many of them and in normal HTTP, they block one another after a certain point.  __HTTP2__ more or less circumvents this blocking problem, essentially reducing the time needed to make a request by the number of requests made.  A possible extension of this is to use [gRPC](https://en.wikipedia.org/wiki/GRPC) (which uses __HTTP2__ under the hood together with [Protobuf](https://en.wikipedia.org/wiki/Protocol_Buffers)) but for now, __HTTP2__ alone works well enough.

## Pitfalls of other routes

- One possible solution is to use an already existing tiling service, like [mapbox](https://www.mapbox.com) for creating/hosting our data or __OpenSeaDragon__ with DeepZoom (or some combination of the two).  However, these services does not support high bit-depth-per-channel images.  To use these then, we could pack 32 bits (for high bit-depth-per-channel microscopy images) across the total 32 bits of the RGBA of a standard image file and then serve that and decode it.  Besides the kludge-y nature of this, we don't save anything really in terms of time as far as transporting the data is concerned, assuming we use `png`.  Additionally, if we attempted to do this with `jpeg` format, it's not even clear how the color-space transformation, DCT, and entropy encoding will react to what is no longer color values, but rather a bit packing scheme.   
- Vector tiles from __mapbox__ are another option, relaying on __Protobuf__, but our data is not vector data.  This raises questions of how we can get the data into that format, and, once it is in that format, why we would not just serve it ourselves over __gRPC__ which appears to be faster.
